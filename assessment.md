How do we know what students know?  How do we know what they have learned?  How can we tell if what teachers are teaching and how they're teaching is actually working?  (Typically) We assess students.  

*How* we assess students, *when* we assess students, *who* assesses students, and what we infer from the assessments can be complex and, at times, incredibly contentious.  That's particularly the case since assessment is often related to grading or standardized testing, and as such, these sorts of assessments are used to make important decisions, such as whether a student will pass a class or graduate from high school, or if a school will continue to receive funding, or if a teacher will remain employed.

But assessment doesn't necessarily mean (and shouldn't solely entail) testing.  And it isn't simply something that happens at the end of the class or end of the school year.  That sort of assessment is known as **summative** and is used to evaluate what students have learned.  That's different -- often in the form and certainly in the purpose -- from **formative** assessments which are given throughout a class or school year and are used to gauge what and how well students are learning.  Formative assessments can (ideally) give learners and teachers feedback on the learning process as it happens, enabling course-correction, remediation, feedback and support as needed.

When we assess students then, we must consider our goals in doing so -- something that in turn should shape how we consider teaching.  What do we want students to learn?  What evidence will we look at to demonstrate that learning?  What criteria will we use to judge this?  All this might seem easier to do when we design assessments that have a right or wrong answer (such as with multiple choice tests) -- but we shouldn't confuse efficiency here with meaningful assessment.

To that end, scholars like [Grant Wiggins](http://grantwiggins.wordpress.com/) have called for "**authentic assessment**" -- that is, assessment that moves away from fill-in-the-blank-type testing and that is realistic to the learner.  After all, grownups rarely if ever have to complete multiple choice tests on the job to see if they're up to par, so why are we so committed to assessing students that way?  Authentic assessment can include open-ended questions, essays, journals, presentations, and e-portfolios.

Authentic assessment also raises the question of *who* is doing the evaluating. Whereas standardized testing is typically done by a machine and traditional testing done by a teacher, other forms of assessment can be done by peers and by the learner her or himself.  By involving students in the assessment process (not just in the measurement itself but in thinking through the very criteria to measure), students have a larger stake in their learning.

But how do we know that these sorts of assessments -- whether done by teachers or by learners or by their peers -- are valid or reliable?  The century-old field of [psychometrics](http://en.wikipedia.org/wiki/Psychometrics) is concerned with both the theory and the design of these sorts of measurements, and has developed approaches like [item-response theory](http://en.wikipedia.org/wiki/Item_response_theory) to create models for assessment.  But even more than longstanding questions about how to create reliable and valid models of assessment, we now face questions too about how we are use assessments to mean more than they're meant to -- whether it's as a predictor for students' future performance or an indicator of teachers' quality.

Assessment, whether in its current form or otherwise, can be seen as a way to control learners. Indeed, as Canadian educator and MOOC pioneer [Dave Cormier argues](http://davecormier.com/edblog/2012/08/16/rhizomatic-learning-and-moocs-assessment/), 

>IF
>
>What we are learning is contextualized by each individual differently, according to their experiences, their understanding and purposes,
>
>AND
>
>The things that are learned are not definite, but flexible and complex
>
>THEN
>
>Assessing what someone ‘knows’ is an act of enforcement of a given point of view, not a(n apolotical) helpful guideline to learning.

Another Canadian educator and connectivist [Stephen Downes has suggested](http://halfanhour.blogspot.ca/2012/08/new-forms-of-assessment-measuring-what.html) that we need to rethink assessment, not by guaging what students know, but by gauging what they contribute:

>Imagine receiving academic credit for contributing well-received resources into open source repositories, whether as software, art, photography, or educational resources. Imagine receiving credit for long-lasting additions to Wikipedia or similar online resources (we would have to fix Wikipedia, as it is now run by a gang of thugs known as 'Wikipedia editors'). We can have wide-ranging and nuanced evaluations of such contributions, not simple grades, but something based on how the content contributed is used and reused across the net (this would have the interesting result that your assessment could continue to go up over time).

Assessments in their current form are something that we do *to* students (and increasingly the entire U.S. school system is focused on standardized assessments as the measurements of how well things are being *done*). But as many scholars argue, this needn't be the only way we think about assessments, and there are ways to re-imagine things so that learners have more responsibility and more agency in the process. 

**Related**:
Cathy Davidson. [How to Crowdsource Grading](http://hastac.org/blogs/cathy-davidson/how-crowdsource-grading)
Edutopia's [Assessment Guide](http://www.edutopia.org/assessment-guide)
[National Center for Research on Evaluation, Standards, and Student Testing](http://www.cse.ucla.edu/)
Doug Belshaw, “[Evaluation: The Absolute Basics](http://dougbelshaw.com/blog/2012/08/16/evaluation-the-absolute-basics/).”