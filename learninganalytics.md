##What Is Learning Analytics?##

Learning analytics is a relatively new field that draws on the collection, measurement and analysis of learners’ data so as to better understand and hopefully improve learning.  

Of course, using education data to make school-related decisions, to modulate instruction, and so on isn’t particularly new. We look at test scores, grades, drop-out rates, attendance and the like all the time in determining what we consider a “good education.” So what makes learning analytics a new field?  Learning analytics is interested in that “traditional” data and more. It’s related to [educational data mining](http://www.educationaldatamining.org/), and it’s also an outgrowth from recent developments in the field of data science, with new tools and new objects of inquiry.

In other words, learning analytics doesn’t just look at quantitative data associated with the classroom. It’s particularly interested in the new types of data we can uncover now with the increasing usage of learning technologies: what’s known as “[data exhaust](http://en.wikipedia.org/wiki/Digital_exhaust)” – that is, all the digital traces from learners’ digital activities: where they go, what they do, what they say, who they’re connected to, who they interact with, what they read, what they write, where they linger, where they bail. (Formally, that’s (social) network analysis, discourse analysis, content analysis, and context analysis, for starters.) Learning analytics is also interested in “big data” and the insights that can be derived from massive amounts of data.

Clearly the amount of data exhaust that all of us leave behind makes for an incredible opportunity for learning analytics. But much of that data remains unexamined, and some of the areas in which learning analytics are seen today remain focused on just a sliver of the tools that students and teachers utilize, giving us only a very partial view into what learners are doing. 

In many cases, the focus remains the learning management system. There, analytics can track: 
How often do students log in? 
How often do they post in discussion boards? 
How do they interact with their peers? 
How long do they stay on the site? 
What do they click on? 
How quickly do professors respond to messages?

If the focus of much of learning analytics remains on the learning management system, the focus of “to what end” might be equally telling. In many cases, the “why we care” about learning analytics involves attempts to boost “student retention” and “course completion.” (The more often students log in to the LMS and the longer they stay there, the more likely they are to pass the class.) That’s not to say that these metrics aren’t important; and while sure, you can say that this is all connected to “learning,” it’s necessarily not the same thing. I realize that retention statistics matter to administrators; I realize it matters to the bottom line of all institutions – for-profit and not-for-profit. (Heck, it matters to students too.) And I realize too that completing a course should mean that students have learned something. But I’d hope the goal of learning analytics isn’t simply on boosting the graduation rate. I hope we can support student learning not just course completion.

Although there’s lots of buzz about educational data and learning analytics, it’s not always clear what we are talking about when we say "learning analytics.” What do we mean by the adjective “learning”?

##Selling Learning Analytics##

If there isn’t a clear discussion, it could be because in most cases the learner voice – or at least the student voice, perhaps – is largely absent from the conversations. That’s not that uncommon, unfortuately, in many an ed-tech discussion.

And unfortunately that might mean that data and analytics will be something we do *to* students, rather than do *for* them or do *with* them. As with much of the “data exhaust” people are currently generating, learners may not have the opportunity to dictate conditions by which analytics about their own learning are even derived. So we should ask: Are we forthcoming with students about the data we collect on them and how we plan to use it? Do students control and do they own their own learning data? Can they dictate who gets to see it? 

This might be an opportunity for the [quantified self movement](http://hackeducation.com/2012/04/30/the-quantified-self-and-learning-analytics/) as it relates to learning analytics, however. The quantified self movement implies if not demands personal ownership and personal control of data. It might even require a personal definition of “learning.” It certainly requires setting personal goals. It means learning to read and build visualizations.

The [2012 Horizon Report](http://net.educause.edu/ir/library/pdf/HR2012.pdf) put the time-to-adoption of learning analytics at two to three years. That’s at an institutional level, mind you, not at the level of our individual readiness as learners to craft our own data dashboards and data visualizations. It’s debatable which will move towards adoption more quickly — individuals or institutions and there are lots of challenges in terms of institutional inertia, vendor lock-in, data ownership, disciplinary silos, let alone the larger questions about what roles learning theory and practice will play in turning educational data into something actionable.
